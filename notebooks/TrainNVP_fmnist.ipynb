{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## notebook for training a RealNVP on the encoded data distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "D-Fe5G8m1FTC",
    "outputId": "e13b192d-d978-43b8-84f1-d6b359f555b1"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib import rcParams\n",
    "import sys\n",
    "import pickle\n",
    "from functools import partial\n",
    "from scipy.linalg import sqrtm\n",
    " \n",
    "plt.rcParams.update({'font.family' : 'lmodern', 'font.size': 16,                                                                                                                                                    \n",
    "                     'axes.labelsize': 16, 'legend.fontsize': 12, \n",
    "                     'xtick.labelsize': 16, 'ytick.labelsize': 16, 'axes.titlesize': 16,\n",
    "                     'axes.linewidth': 1.5}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-AEYmOsH1FTI"
   },
   "outputs": [],
   "source": [
    "import tensorflow_probability as tfp\n",
    "import tensorflow_hub as hub\n",
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GI5FLHJsZLTV"
   },
   "outputs": [],
   "source": [
    "from rnf.model import get_prior, get_posterior, get_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V9oAjWb9ZLTc"
   },
   "outputs": [],
   "source": [
    "import rnf.create_datasets as crd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### specify the parameter file (the only required changes are the latent dim 4<->8,16,32 and the loss AE<->VAE_comp_AE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 126
    },
    "colab_type": "code",
    "id": "SV1Y2txlhckE",
    "outputId": "4f3db526-999c-417b-b1ae-655ef8fa471f"
   },
   "outputs": [],
   "source": [
    "param_file   = 'params_fmnist_-1_4_infoGAN_AE_full_sigma'\n",
    "PROJECT_PATH = \"../\" \n",
    "PARAMS_PATH  = os.path.join(PROJECT_PATH,'params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params   = pickle.load(open(os.path.join(PARAMS_PATH,param_file+'.pkl'),'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(params['plot_dir']):\n",
    "    os.makedirs(params['plot_dir'])\n",
    "if not os.path.isdir(params['data_dir']):\n",
    "    os.makedirs(params['data_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8puPFE90P0aD"
   },
   "outputs": [],
   "source": [
    "generator_path   = os.path.join(params['module_dir'],'decoder')\n",
    "encoder_path     = os.path.join(params['module_dir'],'encoder')\n",
    "nvp_path         = os.path.join(params['module_dir'],'nvp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The RealNVP depth and shift parameter depend on latent dimensionality:\n",
    "latent dim 4: 8, 4  \n",
    "latent dim 8: 10, 4  \n",
    "latent dim 16: 12, 4  \n",
    "latent dim 32: 18, 10   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pB-pfWltZLT6"
   },
   "outputs": [],
   "source": [
    "nvp_depth    = 8\n",
    "shift        = 4\n",
    "indices      = np.arange(params['latent_size'])\n",
    "permutations = [np.random.permutation(indices) for ii in range(nvp_depth)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "igyd5aQ5ZLT-",
    "outputId": "5e4229b3-5b31-4947-9d4e-fb66faa8426b"
   },
   "outputs": [],
   "source": [
    "train_input_fn, eval_input_fn = crd.build_input_fns(params,label=params['class_label'],flatten=False,num_repeat=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 787
    },
    "colab_type": "code",
    "id": "yvTEYw44O_5q",
    "outputId": "eb03d58e-34da-4cac-c0f4-c034d3d47397"
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "lr            = tf.placeholder_with_default(0.001,shape=[])\n",
    "sigma         = tf.placeholder_with_default(0.1,shape=[])\n",
    "sample_size   = tf.placeholder_with_default(params['batch_size'],shape=[])\n",
    "data          = train_input_fn()\n",
    "validdata     = eval_input_fn()\n",
    "encoder       = hub.Module(encoder_path, trainable=False)\n",
    "generator     = hub.Module(generator_path, trainable=False)\n",
    "#nvp_funcs     = hub.Module(nvp_dir, trainable=False)\n",
    "optimizer     = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "\n",
    "prior         = get_prior(params['latent_size'])\n",
    "posterior     = get_posterior(encoder)\n",
    "likelihood    = get_likelihood(generator,params)\n",
    "\n",
    "# inspect the model\n",
    "z_    = posterior(validdata).mean()\n",
    "recon = likelihood(z_).mean()\n",
    "prior_sample = prior.sample(sample_size)\n",
    "x_old   = likelihood(prior_sample).mean()\n",
    "\n",
    "# ### ----------- NVP ----------- ###\n",
    "def init_once(x, name):\n",
    "    return tf.get_variable(name, initializer=x, trainable=False)\n",
    "\n",
    "\n",
    "nvp_size      = [[32,32] for ii in range(nvp_depth)]\n",
    "\n",
    "\n",
    "def get_nvp():\n",
    "    base_dis = get_prior(params['latent_size'])\n",
    "    chain    = []\n",
    "  \n",
    "    perms         = [tfb.Permute(permutation=init_once(permutations[ii],name=\"permutation%d\"%ii)) for ii in range(nvp_depth)]\n",
    "  \n",
    "    for i,s in enumerate(nvp_size):\n",
    "        if i%2==0:\n",
    "            chain.append(perms[i])\n",
    "        if i<shift:\n",
    "            chain.append(tfb.RealNVP(num_masked=params['latent_size']//2,shift_and_log_scale_fn=tfb.real_nvp_default_template(hidden_layers=s,name='nvp%d'%i, shift_only=True)))\n",
    "        else:\n",
    "            chain.append(tfb.RealNVP(num_masked=params['latent_size']//2,shift_and_log_scale_fn=tfb.real_nvp_default_template(hidden_layers=s,name='nvp%d'%i)))\n",
    "    chain.append(perms[i])\n",
    "\n",
    "    nvp = tfd.TransformedDistribution(distribution=base_dis,bijector=tfb.Chain(chain),name='mynvp')\n",
    "                 \n",
    "    return nvp\n",
    "\n",
    "def nvp_module_spec():\n",
    "    z_sample     = tf.placeholder(tf.float32, shape=[None,params['latent_size']])\n",
    "    sample_size  = tf.placeholder(tf.int32, shape=[])\n",
    "    u_sample     = tf.placeholder(tf.float32, shape=[None,params['latent_size']])\n",
    "    nvp_         = get_nvp()\n",
    "    log_prob     = nvp_.log_prob(z_sample)\n",
    "    nvp_sample   = nvp_.sample(sample_size)\n",
    "    nvp_fwd_pass = nvp_.bijector.forward(u_sample)\n",
    "    nvp_bwd_pass = nvp_.bijector.inverse(z_sample)\n",
    "    hub.add_signature(inputs={'z_sample':z_sample,'sample_size':sample_size, 'u_sample':u_sample}\\\n",
    "                      ,outputs={'log_prob':log_prob, 'sample':nvp_sample, 'fwd_pass': nvp_fwd_pass, 'bwd_pass': nvp_bwd_pass})\n",
    "\n",
    "nvp_spec  = hub.create_module_spec(nvp_module_spec)\n",
    "nvp_funcs = hub.Module(nvp_spec, name='nvp_funcs',trainable=True)\n",
    "\n",
    "z         = posterior(data).mean()\n",
    "loss      = -tf.reduce_mean(nvp_funcs({'z_sample':z,'sample_size':1, 'u_sample':np.zeros((1,params['latent_size']))},as_dict=True)['log_prob'])\n",
    "\n",
    "opt_op_nvp= optimizer.minimize(loss)\n",
    "\n",
    "z_valid   = posterior(validdata).mean()\n",
    "loss_valid= -tf.reduce_mean(nvp_funcs({'z_sample':z_valid,'sample_size':1, 'u_sample':np.zeros((1,params['latent_size']))},as_dict=True)['log_prob'])\n",
    "nvp_encoded_valid = -tf.reduce_mean(nvp_funcs({'z_sample':z_valid,'sample_size':1, 'u_sample':np.zeros((1,params['latent_size']))},as_dict=True)['bwd_pass'])\n",
    "nvp_encoded_train = -tf.reduce_mean(nvp_funcs({'z_sample':z,'sample_size':1, 'u_sample':np.zeros((1,params['latent_size']))},as_dict=True)['bwd_pass'])\n",
    "\n",
    "\n",
    "nvp_sample= nvp_funcs({'z_sample':np.zeros((1,params['latent_size'])),'sample_size':1, 'u_sample':prior_sample}, as_dict=True)['fwd_pass']\n",
    "x_new     = likelihood(nvp_sample).mean()\n",
    " \n",
    "# # ---------------------------end train nvp ----------------- #\n",
    "\n",
    "# #comment train nvp above section above and uncomment this for loading the trained module\n",
    "\n",
    "# #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Soh1tnGH1FTW"
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### reconstructions and samples before fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tts2_qqiZLUR"
   },
   "outputs": [],
   "source": [
    "rec, d   = sess.run([recon,validdata])\n",
    "x_sample = sess.run(x_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 247
    },
    "colab_type": "code",
    "id": "vQtp4nklZLUc",
    "outputId": "a317d12d-f2e7-4957-cab9-e2fe35949e6a"
   },
   "outputs": [],
   "source": [
    "#reconstructions\n",
    "jj=1\n",
    "plt.figure(figsize=(4*2,2*2))\n",
    "for ii in range(4):\n",
    "    plt.subplot(2,4,jj)\n",
    "    plt.imshow((d[ii]+0.5).reshape(64,64,-1))\n",
    "    plt.axis('off')\n",
    "    jj+=1\n",
    "    plt.subplot(2,4,jj)\n",
    "    plt.imshow((rec[ii]+0.5).reshape(64,64,-1),vmin=0, vmax=1)\n",
    "    plt.axis('off')\n",
    "    jj+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 194
    },
    "colab_type": "code",
    "id": "i8MQqMXgZLUh",
    "outputId": "2e9ff5a2-3f9e-4fa6-e777-d195d0f7eda6"
   },
   "outputs": [],
   "source": [
    "# samples\n",
    "plt.figure(figsize=(3,3))\n",
    "for ii in range(16):\n",
    "    plt.subplot(4,4,ii+1)\n",
    "    plt.imshow((x_sample[ii]+0.5).reshape(64,64,-1),cmap='gray',vmin=0, vmax=1)\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### to get a crude idea: compare latent space distribution of data with prior "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "uGq9u3q4ZLUn",
    "outputId": "36479428-fe40-447e-dd61-fd10a809e435"
   },
   "outputs": [],
   "source": [
    "z_sample=[]\n",
    "z_sample_valid=[]\n",
    "for ii in range(100):\n",
    "    z_sample+=[sess.run(z)]\n",
    "for ii in range(100):\n",
    "    z_sample_valid+=[sess.run(z_valid)]\n",
    "z_sample=np.asarray(z_sample).reshape((-1,params['latent_size']))\n",
    "z_sample_valid=np.asarray(z_sample_valid).reshape((-1,params['latent_size']))\n",
    "print(z_sample.shape, z_sample_valid.shape)\n",
    "\n",
    "prior_s = sess.run(prior_sample,feed_dict={sample_size:100*params['batch_size']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 177
    },
    "colab_type": "code",
    "id": "11-yOXgtZLUs",
    "outputId": "a1270533-68a1-4523-8096-6a7f05014543"
   },
   "outputs": [],
   "source": [
    "red = 16\n",
    "plt.figure(figsize=((params['latent_size']-red)*3+2,3))\n",
    "for nn in range(params['latent_size']-red):\n",
    "    plt.subplot(1,params['latent_size']-red,nn+1)\n",
    "    plt.scatter(prior_s[:,nn],prior_s[:,nn+1],s=1, label='prior sample') \n",
    "    plt.scatter(z_sample[:,nn],z_sample[:,nn+1],s=1, label='latent space samples') \n",
    "    plt.ylabel('latent dim %d'%nn)\n",
    "    plt.xlabel('latent dim %d'%(nn+1))\n",
    "    plt.tight_layout()\n",
    "    plt.xlim(-3,3)\n",
    "    plt.ylim(-3,3)\n",
    "plt.legend(loc=(1.05,0.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U0xZ_4ActOw9"
   },
   "source": [
    "### Train the realNVP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ggPmR0fs8qEM"
   },
   "outputs": [],
   "source": [
    "nvp_loss = []\n",
    "nvp_vloss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ERmZCW6yBqUR",
    "outputId": "00e5edef-b4a7-4a02-e42c-be23fadf8275"
   },
   "outputs": [],
   "source": [
    "ii     = 0\n",
    "learning_rate = 2e-3\n",
    "\n",
    "#train the nvp for 100 epochs\n",
    "while ii<(100*50000)//params['batch_size']:\n",
    "    _, ll = sess.run([opt_op_nvp,loss],  feed_dict={lr: learning_rate})\n",
    "    nvp_loss+=[ll]\n",
    "    if ii%200==0:\n",
    "        l_ = sess.run(loss_valid,  feed_dict={lr: learning_rate})\n",
    "        print(np.mean(nvp_loss[-100::]),l_)\n",
    "        nvp_vloss+=[l_]\n",
    "    if ii == (50*50000):\n",
    "        print('reducing learning rate')\n",
    "        learning_rate=1e-4\n",
    "    ii+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 292
    },
    "colab_type": "code",
    "id": "bvfbqZJ4RX9x",
    "outputId": "44fdbfd6-4225-406f-f222-46ab5114714c"
   },
   "outputs": [],
   "source": [
    "#plot the loss function\n",
    "plt.figure()\n",
    "plt.plot(np.convolve(nvp_loss[::],np.ones((100))/100., mode='valid'),label='training loss (smoothed)')\n",
    "plt.plot(np.arange(0,len(nvp_vloss)*197,200),np.convolve(nvp_vloss,np.ones((3))/3., mode='valid'),label='validation loss' )\n",
    "plt.xlabel('# iteration')\n",
    "plt.ylabel('RealNVP loss')\n",
    "plt.legend()\n",
    "plt.ylim(-22,8)\n",
    "plt.savefig(os.path.join(params['plot_dir'],'RealNVP_loss.pdf'),bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### compare samples before and after realNVP fit with reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YWmT0AjAeOKs"
   },
   "outputs": [],
   "source": [
    "s_no_nvp, s_nvp = sess.run([x_old,x_new])\n",
    "z_sample_nvp    = sess.run(nvp_sample,feed_dict={sample_size:100*params['batch_size']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 548
    },
    "colab_type": "code",
    "id": "RgjwDx6JZLVH",
    "outputId": "8a200255-50f0-4c47-8f18-6319dfe8b665"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3,3))\n",
    "plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.05, hspace=0.05)\n",
    "for ii in range(16):\n",
    "    plt.subplot(4,4,ii+1)\n",
    "    plt.imshow((s_nvp[ii]+0.5).reshape(64,64,-1),cmap='gray',vmin=0, vmax=1)\n",
    "    plt.axis('off')\n",
    "plt.savefig(os.path.join(params['plot_dir'],'NVP_samples.pdf'),bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.figure(figsize=(3,3))\n",
    "plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.05, hspace=0.05)\n",
    "for ii in range(16):\n",
    "    plt.subplot(4,4,ii+1)\n",
    "    plt.imshow((s_no_nvp[ii]+0.5).reshape(64,64,-1),cmap='gray',vmin=0, vmax=1)\n",
    "    plt.axis('off')\n",
    "plt.savefig(os.path.join(params['plot_dir'],'raw_samples.pdf'),bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.figure(figsize=(3,3))\n",
    "plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.05, hspace=0.05)\n",
    "for ii in range(16):\n",
    "    plt.subplot(4,4,ii+1)\n",
    "    plt.imshow((rec[ii]+0.5).reshape(64,64,-1),cmap='gray',vmin=0, vmax=1)\n",
    "    plt.axis('off')\n",
    "plt.savefig(os.path.join(params['plot_dir'],'reconstructions.pdf'),bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### compare the distribution of realNVP samples with encoded data distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 189
    },
    "colab_type": "code",
    "id": "Wu9VWnXmZLVJ",
    "outputId": "2381a87f-6ebf-44c2-daa0-79c600b03917"
   },
   "outputs": [],
   "source": [
    "red = 24\n",
    "plt.figure(figsize=((params['latent_size']-red)*3+2,3))\n",
    "for nn in range(params['latent_size']-red):\n",
    "    plt.subplot(1,params['latent_size']-red,nn+1)\n",
    "    plt.scatter(prior_s[:,nn],prior_s[:,nn+1],s=2, label='prior samples') \n",
    "    plt.scatter(z_sample[:,nn],z_sample[:,nn+1],s=2, label='encoded data')  \n",
    "    plt.scatter(z_sample_nvp[:,nn],z_sample_nvp[:,nn+1],s=2, label='RealNVP samples') \n",
    "    plt.scatter(z_sample_valid[:,nn],z_sample_valid[:,nn+1],s=1, label='encoded data valid')\n",
    "    plt.ylabel('latent dim %d'%nn)\n",
    "    plt.xlabel('latent dim %d'%(nn+1))\n",
    "    plt.tight_layout()\n",
    "    plt.xlim(-5,5)\n",
    "    plt.ylim(-5,5)\n",
    "plt.legend(loc=(1.02,0.4))\n",
    "plt.savefig(os.path.join(params['plot_dir'],'latent_space_samples.pdf'),bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### save the realNVP model as a module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TYtuOw8uB-aO"
   },
   "outputs": [],
   "source": [
    "if not os.path.isdir(nvp_path):\n",
    "    os.makedirs(nvp_path)\n",
    "nvp_funcs.export(nvp_path,sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XdDkZoh8ZLVT"
   },
   "outputs": [],
   "source": [
    "pickle.dump(params,open(os.path.join(PARAMS_PATH,param_file+'.pkl'),'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "TrainNVP and measure Reconstruction Noise.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "my_tflow",
   "language": "python",
   "name": "my_tflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
